  0%|                                                                                                                                         | 0/746 [00:00<?, ?it/s]/export/project/jzhanggr/miniconda3/envs/lmflow/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/export/project/jzhanggr/miniconda3/envs/lmflow/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/export/project/jzhanggr/miniconda3/envs/lmflow/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|▏                                                                                                                              | 1/746 [00:17<3:37:00, 17.48s/it]
{'loss': 16.8438, 'grad_norm': 3635.1146068026305, 'learning_rate': 4.347826086956522e-07, 'epoch': 0.0}

  0%|▎                                                                                                                              | 2/746 [00:26<2:36:34, 12.63s/it]

  0%|▌                                                                                                                              | 3/746 [00:35<2:13:40, 10.79s/it]
  1%|▋                                                                                                                              | 4/746 [00:43<2:02:30,  9.91s/it]


 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 39/42 [00:05<00:00,  7.98it/s]


  1%|▊                                                                                                                              | 5/746 [00:59<2:28:12, 12.00s/it]

  1%|█                                                                                                                              | 6/746 [01:07<2:11:46, 10.69s/it]

  1%|█▏                                                                                                                             | 7/746 [01:15<2:00:23,  9.77s/it]

  1%|█▎                                                                                                                             | 8/746 [01:23<1:53:33,  9.23s/it]
{'loss': 30.0625, 'grad_norm': 6899.795811346974, 'learning_rate': 3.4782608695652175e-06, 'epoch': 0.02}


 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 41/42 [00:05<00:00,  8.29it/s]

{'eval_loss': 20.138931274414062, 'eval_runtime': 6.9872, 'eval_samples_per_second': 47.515, 'eval_steps_per_second': 6.011, 'epoch': 0.02}

  1%|█▌                                                                                                                             | 9/746 [01:38<2:16:40, 11.13s/it]

  1%|█▋                                                                                                                            | 10/746 [01:46<2:04:35, 10.16s/it]

  1%|▏            | 11/746 [01:54<1:56:23,  9.50s/it]
  2%|▏            | 12/746 [02:03<1:50:48,  9.06s/it]



100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:06<00:00,  3.31it/s]

{'eval_loss': 1.4579136371612549, 'eval_runtime': 7.0789, 'eval_samples_per_second': 46.9, 'eval_steps_per_second': 5.933, 'epoch': 0.03}

  2%|██▏                                                                                                                           | 13/746 [02:20<2:21:24, 11.58s/it]

  2%|██▎                                                                                                                           | 14/746 [02:28<2:08:20, 10.52s/it]

  2%|██▌                                                                                                                           | 15/746 [02:36<1:59:10,  9.78s/it]
  2%|▎            | 16/746 [02:45<1:54:10,  9.38s/it]



 74%|███████████▊    | 31/42 [00:04<00:01,  8.27it/s]
{'eval_loss': 5.747929096221924, 'eval_runtime': 7.0556, 'eval_samples_per_second': 47.055, 'eval_steps_per_second': 5.953, 'epoch': 0.04}

  2%|▎            | 17/746 [03:00<2:16:06, 11.20s/it]

  2%|▎            | 18/746 [03:08<2:04:58, 10.30s/it]

  3%|▎            | 19/746 [03:16<1:56:56,  9.65s/it]
  3%|▎            | 20/746 [03:24<1:50:46,  9.16s/it]



 83%|█████████████▎  | 35/42 [00:04<00:00,  8.51it/s]

  3%|▎            | 21/746 [03:40<2:13:26, 11.04s/it]

  3%|▍            | 22/746 [03:48<2:01:59, 10.11s/it]

  3%|▍            | 23/746 [03:56<1:54:01,  9.46s/it]

  3%|▍            | 24/746 [04:03<1:48:12,  8.99s/it]
{'loss': 0.3701, 'grad_norm': 161.1928496935515, 'learning_rate': 9.986168741355464e-06, 'epoch': 0.06}


 98%|███████████████▌| 41/42 [00:05<00:00,  8.03it/s]


  3%|▍            | 25/746 [04:19<2:11:45, 10.96s/it]

  3%|▍            | 26/746 [04:27<2:00:36, 10.05s/it]
{'loss': 1.2188, 'grad_norm': 514.0588369611194, 'learning_rate': 9.95850622406639e-06, 'epoch': 0.07}

  4%|▍            | 27/746 [04:35<1:52:38,  9.40s/it]
  4%|▍            | 28/746 [04:43<1:47:25,  8.98s/it]



100%|████████████████| 42/42 [00:06<00:00,  3.38it/s]

{'eval_loss': 4.671122074127197, 'eval_runtime': 7.0044, 'eval_samples_per_second': 47.399, 'eval_steps_per_second': 5.996, 'epoch': 0.08}

  4%|▌            | 29/746 [04:58<2:09:30, 10.84s/it]

  4%|▌            | 30/746 [05:06<1:59:01,  9.97s/it]

  4%|▌            | 31/746 [05:14<1:51:44,  9.38s/it]
  4%|▌            | 32/746 [05:22<1:46:36,  8.96s/it]


 93%|██████████████▊ | 39/42 [00:05<00:00,  7.82it/s]

{'eval_loss': 9.107680320739746, 'eval_runtime': 7.0333, 'eval_samples_per_second': 47.204, 'eval_steps_per_second': 5.972, 'epoch': 0.09}

  4%|▌            | 33/746 [05:37<2:09:33, 10.90s/it]

  5%|▌            | 34/746 [05:45<1:59:07, 10.04s/it]

  5%|▌            | 35/746 [05:53<1:51:43,  9.43s/it]

  5%|▋            | 36/746 [06:01<1:46:44,  9.02s/it]


 98%|███████████████▌| 41/42 [00:05<00:00,  8.26it/s]

{'eval_loss': 12.191265106201172, 'eval_runtime': 7.013, 'eval_samples_per_second': 47.34, 'eval_steps_per_second': 5.989, 'epoch': 0.1}

  5%|▋            | 37/746 [06:17<2:09:22, 10.95s/it]

  5%|▋            | 38/746 [06:25<1:59:32, 10.13s/it]


  5%|▋            | 40/746 [06:42<1:47:45,  9.16s/it]
{'loss': 2.7812, 'grad_norm': 804.8958743783625, 'learning_rate': 9.764868603042877e-06, 'epoch': 0.11}


 98%|███████████████▌| 41/42 [00:05<00:00,  8.07it/s]

{'eval_loss': 2.4711031913757324, 'eval_runtime': 7.0656, 'eval_samples_per_second': 46.988, 'eval_steps_per_second': 5.944, 'epoch': 0.11}

  5%|██████▉                                                                                                                       | 41/746 [06:57<2:11:29, 11.19s/it]

  6%|███████                                                                                                                       | 42/746 [07:06<2:03:04, 10.49s/it]


  6%|███████▍                                                                                                                      | 44/746 [07:24<1:52:56,  9.65s/it]
{'loss': 0.7949, 'grad_norm': 552.8026064059122, 'learning_rate': 9.709543568464731e-06, 'epoch': 0.12}



 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 41/42 [00:07<00:00,  7.97it/s]

{'eval_loss': 77.86445617675781, 'eval_runtime': 9.0123, 'eval_samples_per_second': 36.839, 'eval_steps_per_second': 4.66, 'epoch': 0.12}


  6%|███████▊                                                                                                                      | 46/746 [07:54<2:19:40, 11.97s/it]
{'loss': 5.3281, 'grad_norm': 1666.089477041318, 'learning_rate': 9.681881051175658e-06, 'epoch': 0.12}

  6%|███████▉                                                                                                                      | 47/746 [08:03<2:10:35, 11.21s/it]

  6%|████████                                                                                                                      | 48/746 [08:13<2:04:41, 10.72s/it]




 98%|███████████████▌| 41/42 [00:09<00:00,  2.93it/s]

{'eval_loss': 3.249058723449707, 'eval_runtime': 12.7449, 'eval_samples_per_second': 26.05, 'eval_steps_per_second': 3.295, 'epoch': 0.13}


  7%|▊            | 50/746 [08:52<2:49:07, 14.58s/it]

  7%|▉            | 51/746 [09:06<2:45:59, 14.33s/it]

  7%|▉            | 52/746 [09:16<2:30:31, 13.01s/it]
{'loss': 1.5781, 'grad_norm': 588.1174233089413, 'learning_rate': 9.598893499308438e-06, 'epoch': 0.14}





 98%|███████████████▌| 41/42 [00:09<00:00,  3.34it/s]

  7%|▉            | 53/746 [09:41<3:14:52, 16.87s/it]
{'loss': 3.0156, 'grad_norm': 1268.5792847163034, 'learning_rate': 9.585062240663902e-06, 'epoch': 0.14}

  7%|▉            | 54/746 [09:51<2:50:01, 14.74s/it]


  8%|▉            | 56/746 [10:19<2:46:01, 14.44s/it]
{'loss': 1.1953, 'grad_norm': 701.5510004581965, 'learning_rate': 9.543568464730292e-06, 'epoch': 0.15}





 98%|███████████████▌| 41/42 [00:08<00:00,  8.24it/s]

  8%|▉            | 57/746 [10:45<3:25:27, 17.89s/it]

  8%|█████████▊                                                                                                                    | 58/746 [10:54<2:56:09, 15.36s/it]

  8%|█████████▉                                                                                                                    | 59/746 [11:10<2:56:59, 15.46s/it]

  8%|██████████▏                                                                                                                   | 60/746 [11:24<2:53:11, 15.15s/it]
{'loss': 0.4805, 'grad_norm': 273.9042956528954, 'learning_rate': 9.488243430152144e-06, 'epoch': 0.16}






 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 41/42 [00:12<00:00,  7.50it/s]


  8%|██████████▎                                                                                                                   | 61/746 [11:54<3:43:46, 19.60s/it]
